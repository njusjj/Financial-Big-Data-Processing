# 实验三 171870632 史健均

###阶段一

**使用的python写的MapReduce**：

按照省份作为key，来分类；

value按照（itemid+action）来记录；

在Reduce阶段，将按照不同省份统计，该省份的不同商品关注量和购买量，建立一个字典结果如下：

dict={}

dict[key]=[{},{}]

key的值为一个包含双字典的列表，第一个字典记录关注度，第二个字典记录购买量。

结果如图：

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果1.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果2.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果3.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果4.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果5.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果6.png)

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/阶段1结果7.png)

### 阶段二：

配置hive；

hadoop版本：2.8.5

hive版本：2.3.6

版本匹配。

下载hive-2.3.6：

> https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz

解压压缩包：

> tar -xzvf apache-hive-2.3.6-bin.tar.gz /Users/shijianjun

在~/.zshrc里面加上环境变量：

> export HIVE_HOME=/Users/shijianjun/hive-2.3.6
>
> export PATH=\$HIVE_HOME/bin:$PATH

修改conf下的hive-env.sh.template中的HADOOP_HOME，并存为后缀为xml的文件：

> HADOOP_HOME=/Users/shijianjun/hadoop-2.8.5

修改hive-default.xml.template为新的hive-site.xml：

> <?xml version="1.0" encoding="UTF-8" standalone="no"?>
> <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
> <configuration>
>     <property>
> 		<name>javax.jdo.option.ConnectionURL</name>
> 		<value>jdbc:derby:;databaseName=metastore_db;create=true</value>
> 	</property>
> 	<property>
> 		<name>hive.metastore.warehouse.dir</name>
> 		<value>/hive/warehouse</value>
> 	</property>
> </configuration>

启动Hive，初始化MetaStore，进入bin：

> schematool -dbType derby -initSchema

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/初始化hive.png)

启动Hive

> hive

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/hive启动.png)

####阶段2实验开始：

创建表：

> hive> CREATE TABLE  million_user_log (user_id INT, item_id INT, cat_id INT, merchant_id INT, brand_id INT, month INT, day INT, action INT, age_range INT, gender INT, province STRING) row format delimited fields terminated by ',';

将文件插入数据表中：

> hive> LOAD DATA LOCAL INPATH '/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/双十一数据/million_user_log.csv' INTO TABLE million_user_log;

#####实验第一个问题：

取购买的数据，然后计数人数，并去重：

> hive> SELECT count(DISTINCT user_id) FROM million_user_log WHERE action=2;

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/第一个问题.png)

#####实验第二个问题：

先取购买行为数据，然后按照性别分为三组，然后对每一组进行计数，对user_id去重

> hive> SELECT gender,count(DISTINCT user_id) FROM million_user_log WHERE action=2 GROUP BY gender;

![avator](/Users/shijianjun/学习资料/金融大数据处理/作业/实验3/第二个问题.png)

#####实验第三个问题：

首先

> hive> SELECT brand_id,count(brand_id) number FROM million_user_log WHERE action=0 GROUP BY brand_id ORDER BY number DESC LIMIT 10;